{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptocurrency prediction with Recurrent Neural Networks\n",
    "Source:\n",
    "1. https://pythonprogramming.net/crypto-rnn-model-deep-learning-python-tensorflow-keras/?completed=/balancing-rnn-data-deep-learning-python-tensorflow-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "It is seen that there are 6 columns, including 'time', 'low', 'high', 'open', 'close' and 'volume'. We decidede that only the closing price and the volumn are important features for predicting the future price. Thus, we have to combine the closing price and the volume of the 4 different crypto currencies.\n",
    "\n",
    "Tips\n",
    "- to rename the headers of the columns, one can use: df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'})\n",
    "- to set a column as the new index, one can use df.set_index\n",
    "- replace gaps in data with previously known values, one can use df.fillna(method=\"ffill\", inplace=True)\n",
    "- Some knowledge regarding f strings https://realpython.com/python-f-strings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         time        low       high       open      close      volume\n",
      "0  1528968660  96.580002  96.589996  96.589996  96.580002    9.647200\n",
      "1  1528968720  96.449997  96.669998  96.589996  96.660004  314.387024\n",
      "2  1528968780  96.470001  96.570000  96.570000  96.570000   77.129799\n",
      "3  1528968840  96.449997  96.570000  96.570000  96.500000    7.216067\n",
      "4  1528968900  96.279999  96.540001  96.500000  96.389999  524.539978\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"input/LTC-USD.csv\", names=['time', 'low', 'high', 'open', 'close', 'volume'])\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            BTC-USD_close  BTC-USD_volume  LTC-USD_close  LTC-USD_volume  \\\n",
      "time                                                                       \n",
      "1528968720    6487.379883        7.706374      96.660004      314.387024   \n",
      "1528968780    6479.410156        3.088252      96.570000       77.129799   \n",
      "1528968840    6479.410156        1.404100      96.500000        7.216067   \n",
      "1528968900    6479.979980        0.753000      96.389999      524.539978   \n",
      "1528968960    6480.000000        1.490900      96.519997       16.991997   \n",
      "\n",
      "            BCH-USD_close  BCH-USD_volume  ETH-USD_close  ETH-USD_volume  \n",
      "time                                                                      \n",
      "1528968720     870.859985       26.856577      486.01001       26.019083  \n",
      "1528968780     870.099976        1.124300      486.00000        8.449400  \n",
      "1528968840     870.789978        1.749862      485.75000       26.994646  \n",
      "1528968900     870.000000        1.680500      486.00000       77.355759  \n",
      "1528968960     869.989990        1.669014      486.00000        7.503300  \n"
     ]
    }
   ],
   "source": [
    "main_df = pd.DataFrame() # begin empty\n",
    "\n",
    "ratios = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]  # the 4 ratios we want to consider\n",
    "for ratio in ratios:  # begin iteration\n",
    "    dataset = f'input/{ratio}.csv'  # get the full path to the file.\n",
    "    df = pd.read_csv(dataset, names=['time', 'low', 'high', 'open', 'close', 'volume'])  # read in specific file\n",
    "\n",
    "    # rename volume and close to include the ticker \n",
    "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
    "    df.set_index(\"time\", inplace=True)  # set time as index so we can join them on this shared time\n",
    "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]  # ignore the other columns besides price and volume\n",
    "\n",
    "    if len(main_df)==0:  # if the dataframe is empty\n",
    "        main_df = df  # then it's just the current df\n",
    "    else:  # otherwise, join this data to the main one\n",
    "        main_df = main_df.join(df)\n",
    "\n",
    "main_df.fillna(method=\"ffill\", inplace=True)\n",
    "main_df.dropna(inplace=True)\n",
    "print(main_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create target\n",
    "Our ojective is to predict Litecoin price. I'd like to go with a sequence length of 60, and a future prediction out of 3. Which means that using the past 60 minutes to predic something in the next 3 minutes. However, we sould be more clear what we would like to predict. What we like to do is that if price goes up in 3 minutes, then it's a buy. If it goes down in 3 minutes, not buy/sell.\n",
    "\n",
    "Tips\n",
    "- map() function returns a list of the results after applying the given function to each item of a given iterable (list, tuple etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(current, future):\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN\n",
    "FUTURE_PERIOD_PREDICT = 3  # how far into the future are we trying to predict?\n",
    "RATIO_TO_PREDICT = \"LTC-USD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
    "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            BTC-USD_close  BTC-USD_volume  LTC-USD_close  LTC-USD_volume  \\\n",
      "time                                                                       \n",
      "1528968720    6487.379883        7.706374      96.660004      314.387024   \n",
      "1528968780    6479.410156        3.088252      96.570000       77.129799   \n",
      "1528968840    6479.410156        1.404100      96.500000        7.216067   \n",
      "\n",
      "            BCH-USD_close  BCH-USD_volume  ETH-USD_close  ETH-USD_volume  \\\n",
      "time                                                                       \n",
      "1528968720     870.859985       26.856577      486.01001       26.019083   \n",
      "1528968780     870.099976        1.124300      486.00000        8.449400   \n",
      "1528968840     870.789978        1.749862      485.75000       26.994646   \n",
      "\n",
      "               future  target  \n",
      "time                           \n",
      "1528968720  96.389999       0  \n",
      "1528968780  96.519997       0  \n",
      "1528968840  96.440002       0  \n"
     ]
    }
   ],
   "source": [
    "print(main_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now we have created a new data frame that includes the information we only needed, which is the closing prices and the volumn of the 4 indecies plus the a) \"future price\" and b) \"direction\" of the index that we are predicting. In fact, in this case we only care about the direction, where we treat this problem as a classification problem, so we will drop the future price column later on. One could also treat this problem as a regression problem, if that's the case, one could drop the direction column(which is our target right now)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and Data balencing\n",
    "- One could normalize the dtat using sklearn's module: preprocessing.scale, note that one sould not normalize the target\n",
    "- Deque is preferred over list in the cases where we need quicker append and pop operations from both the ends of container. We make sue that our sequence contains only 60 datapoints(minutes). This is like a moving window that moves one step at a time and includes 60 points in the window.\n",
    "- data balencing improves the learning efficiency of the model. We first could the sequences with target = 1 and target = 0. Next, we select the same number from each catogory, which means that if we picked 1000 sequencies with target =1, we also have to pick 1000 sequencies with target =0. To get the maximum sequencies, we count the numers of diferent target, choose all the sequencies of targets with lowernumbers, and choose the same number of sequencies from the target with higher numbers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # a small demo of deque\n",
    "# a_list = deque(maxlen=3)\n",
    "# a_list.append('a')\n",
    "# print(a_list)\n",
    "# a_list.append('b')\n",
    "# print(a_list)\n",
    "# a_list.append('c')\n",
    "# print(a_list)\n",
    "# a_list.append('d')\n",
    "# print(a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing  \n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    df = df.drop(\"future\", 1)  # don't need this anymore.\n",
    "\n",
    "    for col in df.columns:  # go through all of the columns\n",
    "        if col != \"target\":  # normalize all ... except for the target itself!\n",
    "            df[col] = df[col].pct_change()  # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\n",
    "            df.dropna(inplace=True)  # remove the nas created by pct_change\n",
    "            df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.\n",
    "\n",
    "    df.dropna(inplace=True)  # cleanup again... jic.\n",
    "\n",
    "\n",
    "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
    "    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
    "\n",
    "    for i in df.values:  # i will be the columns in the data frame\n",
    "        prev_days.append([n for n in i[:-1]])  # store values in the column i but the target\n",
    "        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])  # append the target\n",
    "\n",
    "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
    "    \n",
    "    #Balencing the data\n",
    "\n",
    "    buys = []  # list that will store our buy sequences and targets\n",
    "    sells = []  # list that will store our sell sequences and targets\n",
    "\n",
    "    for seq, target in sequential_data:  # iterate over the sequential data\n",
    "        if target == 0:  # if it's a \"not buy\"\n",
    "            sells.append([seq, target])  # append to sells list\n",
    "        elif target == 1:  # otherwise if the target is a 1...\n",
    "            buys.append([seq, target])  # it's a buy!\n",
    "\n",
    "    random.shuffle(buys)  # shuffle the buys\n",
    "    random.shuffle(sells)  # shuffle the sells!\n",
    "\n",
    "    lower = min(len(buys), len(sells))  # what's the shorter length?\n",
    "\n",
    "    buys = buys[:lower]  # make sure both lists are only up to the shortest length.\n",
    "    sells = sells[:lower]  # make sure both lists are only up to the shortest length.\n",
    "\n",
    "    sequential_data = buys+sells  # add them together\n",
    "    random.shuffle(sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other.\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for seq, target in sequential_data:  # going over our new sequential data\n",
    "        X.append(seq)  # X is the sequences\n",
    "        y.append(target)  # y is the targets/labels (buys vs sell/notbuy)\n",
    "\n",
    "    return np.array(X), y  # return X and y...and make X a numpy array!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = sorted(main_df.index.values)  # get the times\n",
    "last_5pct = sorted(main_df.index.values)[-int(0.05*len(times))]  # get the last 5% of the times\n",
    "\n",
    "validation_main_df = main_df[(main_df.index >= last_5pct)]  # make the validation data where the index is in the last 5%\n",
    "main_df = main_df[(main_df.index < last_5pct)]  # now the main_df is all the data up to the last 5%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = preprocess_df(main_df) \n",
    "validation_x, validation_y = preprocess_df(validation_main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 77922 validation: 3860\n",
      "Dont buys: 38961, buys: 38961\n",
      "VALIDATION Dont buys: 1930, buys: 1930\n"
     ]
    }
   ],
   "source": [
    "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
    "print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
    "print(f\"VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "77922 sequencies with 60 timesteps in a single sequece, each sequence containe 8 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77922, 60, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Phase\n",
    "\n",
    "Source:\n",
    "\n",
    "Input of LSTM\n",
    "1. https://medium.com/@shivajbd/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e\n",
    "2. https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell\n",
    "3. https://datascience.stackexchange.com/questions/12964/what-is-the-meaning-of-the-number-of-units-in-the-lstm-cell\n",
    "4. https://datascience.stackexchange.com/questions/20413/clarification-on-the-keras-recurrent-unit-cell\n",
    "\n",
    "Crossentropy loss\n",
    "1. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "EPOCHS = 10  # how many passes through our data\n",
    "BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"  # a unique name for the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lovem\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "\n",
    "# the model check point saves the model when ever it improves its accuracy\n",
    "# soruce\n",
    "# https://machinelearningmastery.com/check-point-deep-learning-models-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf; print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())  #normalizes activation outputs, same reason you want to normalize your input data.\n",
    "\n",
    "model.add(CuDNNLSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(CuDNNLSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
    "checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 77922 samples, validate on 3860 samples\n",
      "Epoch 1/10\n",
      "77922/77922 [==============================] - 33s 426us/step - loss: 0.7112 - acc: 0.5252 - val_loss: 0.6862 - val_acc: 0.5433- loss: 0.712 - ETA: 0s - loss: 0.7117 - ac\n",
      "Epoch 2/10\n",
      "77922/77922 [==============================] - 30s 388us/step - loss: 0.6854 - acc: 0.5531 - val_loss: 0.6855 - val_acc: 0.5539 - ETA: 1s - loss: 0.6853 - acc: 0.5 - ETA: 0s - loss: 0.68\n",
      "Epoch 3/10\n",
      "77922/77922 [==============================] - 30s 390us/step - loss: 0.6823 - acc: 0.5602 - val_loss: 0.6763 - val_acc: 0.5697\n",
      "Epoch 4/10\n",
      "77922/77922 [==============================] - 31s 393us/step - loss: 0.6814 - acc: 0.5616 - val_loss: 0.6764 - val_acc: 0.5775\n",
      "Epoch 5/10\n",
      "77922/77922 [==============================] - 30s 388us/step - loss: 0.6793 - acc: 0.5697 - val_loss: 0.6817 - val_acc: 0.5637\n",
      "Epoch 6/10\n",
      "77922/77922 [==============================] - 31s 396us/step - loss: 0.6768 - acc: 0.5719 - val_loss: 0.6721 - val_acc: 0.5842\n",
      "Epoch 7/10\n",
      "77922/77922 [==============================] - 30s 391us/step - loss: 0.6745 - acc: 0.5793 - val_loss: 0.6743 - val_acc: 0.58370.6745 - acc: 0.579\n",
      "Epoch 8/10\n",
      "77922/77922 [==============================] - 31s 400us/step - loss: 0.6704 - acc: 0.5872 - val_loss: 0.6753 - val_acc: 0.5829\n",
      "Epoch 9/10\n",
      "77922/77922 [==============================] - 31s 393us/step - loss: 0.6655 - acc: 0.5962 - val_loss: 0.6784 - val_acc: 0.5780\n",
      "Epoch 10/10\n",
      "77922/77922 [==============================] - 31s 396us/step - loss: 0.6599 - acc: 0.6047 - val_loss: 0.6744 - val_acc: 0.5860\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(validation_x, validation_y),\n",
    "    callbacks=[tensorboard, checkpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6744038399637055\n",
      "Test accuracy: 0.586010362756067\n"
     ]
    }
   ],
   "source": [
    "# Score model\n",
    "score = model.evaluate(validation_x, validation_y, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "# Save model\n",
    "model.save(\"models/{}\".format(NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to imporve the acuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Improve Performance With Data\n",
    "1. Get More Data.\n",
    "2. Invent More Data- data augmentation,especially useful for image classification.\n",
    "3. Rescale Your Data- Rescale your data to the bounds of your activation functions. This help the model to learn faster with the same epochs.\n",
    "4. Transform Your Data- a) changing data distribution-skewed Gaussian> Box-Cox transform;exponential distribution>log transform. b) Pre-process data with a projection method like PCA\n",
    "5. Feature Selection-PCA, Univariate Selection, Recursive Feature Elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Improve Performance With Algorithms\n",
    "This method is quite limited, because one can search literatures for approaches to specific problems. If there are different methods, one can try all of them. One could also change the resampling methods used in the model. This is by adjusting how you split the data and which part you use for training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Improve Performance With Algorithm Tuning (or Hyperparameter tuning)\n",
    "\n",
    "1. Diagnostics-The general way. Is the model overfitting or underfitting? This could be solved by cross validation. (For a lot of applications, we couldnt do this, such as time series prediction.)\n",
    "\n",
    "2. Weight Initialization.\n",
    "\n",
    "3. Learning Rate-Lr is one of the hyperparameters in almost all ML models, we see the LR rate in the optimisers. ie. tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "4. Activation Functions(not recommended).\n",
    "\n",
    "5. Network Topology- This is for neural nets, try tuning the hidden layers and hidden units.\n",
    "\n",
    "6. Batches and Epochs-Try a grid search of different mini-batch sizes (8, 16, 32, â€¦). Try training for a few epochs and for a heck of a lot of epochs.\n",
    "\n",
    "7. Regularization.Grid search different dropout percentage, Weight decay, penalties that can be applied such as L1 and L2.\n",
    "\n",
    "8. Optimization and Loss. We chose the adam optimizer(tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)). One could choose a optimizer from Adam, SGD,...etc. For the loss function, we chose the sparse_categorical_crossentropy (loss='sparse_categorical_crossentropy'). In fact there are a various choices for the loss function depending on different applications.\n",
    "\n",
    "\n",
    "source: \n",
    "- https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/\n",
    "- https://machinelearningmastery.com/improve-deep-learning-performance/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Why cross validation ins not used in RNNs?\n",
    "\n",
    "Standard cross validation takes the data sample, leaves a part out, trains the model on the rest, the trains the model on a different set of the data when a different section has been left out, repeat until you've covered the entire dataset.\n",
    "\n",
    "This is because inmost Rnn applications are time related, these kind of dataset are auto-correlated most of the time, ie they depend on the order of events.\n",
    "\n",
    "Resampling techniques such k-fold cross validation would not work in these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Improve Performance With Ensembles\n",
    "\n",
    "Source:\n",
    "- https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to select features in time series prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lag Features, Rolling Window Statistics,Expanding Window Statistics\n",
    "Source:\n",
    "https://machinelearningmastery.com/basic-feature-engineering-time-series-data-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
